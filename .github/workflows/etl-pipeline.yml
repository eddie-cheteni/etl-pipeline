# Workflow name
name: data-pipeline-workflow

# Trigger the workflow
on:
  # push: # uncomment to run on push
  schedule:
    - cron: "/15 * * * *" # run every minute
  workflow_dispatch:  # manual triggers

# Jobs
jobs:
  run-data-pipeline:
    runs-on: windows-latest
    steps:
    # 1. Pull repo content
      - name: Checkout repo content
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}  # Use the PAT instead of the default GITHUB_TOKEN
      
    # 2. Setup python
      - name: Setup python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
          cache: 'pip'

      #  3. Install dependencies
      - name: Install dependencies
        run: pip install -r requirements.txt

      # 4. Run data pipeline
      - name: Run data pipeline
        env:
          PERSONAL_ACCESS_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
          HOSTNAME: ${{ secrets.HOSTNAME }} # import API key
          PASSWORD: ${{ secrets.PASSWORD }} # import API key
          PORT: ${{ secrets.PORT }} # import API key
          SFTP_HOST: ${{ secrets.SFTP_HOST }} # import API key
          USER_NAME: ${{ secrets.USER_NAME }} # import API key  
          FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }} # import API key
          FTP_USERNAME: ${{ secrets.FTP_USERNAME }} # import API key

        run: python etl_pipeline.py # run data pipeline
      
      # 5. Checking changes
      - name: Check for changes # create env variable indicating if any changes were made
        id: git-check
        run: |
          git config user.name 'github-actions'
          git config user.email 'github-actions@github.com'
          git add .
          git diff --staged --quiet || echo "changes=true" >> $GITHUB_ENV 
      - name: Commit and push if changes
        if: env.changes == 'true' # if changes made push new data to repo
        run: |
          git commit -m "update data"
          git push